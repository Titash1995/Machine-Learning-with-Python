{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reference:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/classify-butterfly-images-with-deep-learning-in-keras-b3101fe0f98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/api/preprocessing/image/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn-procedure.png\" height=200 width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.gif\" height=200 width=400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification of Dogs and Cats images using CNN\n",
    "\n",
    "As the dataset is too large to upload, here's a link to download the dataset in the .zip format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/chetankv/dogs-cats-images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augementation occurs when you create new data based on modifications of your existing data.  \n",
    "##### In our case the **data** will be images\n",
    "#####  Data Augementationon images would include transformations like: \n",
    "#####  Flipping the image either horizontally / vertically\n",
    "#####  Rotating the image\n",
    "#####  Zooming in or out of the image\n",
    "#####  Varying color\n",
    "#####  ...AND MANY MORE   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating images for the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # Normalization\n",
    "        shear_range=0.2, \n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code is to extract the images from the directory. \n",
    "#### We need need to connect **train_datagen** object to our training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',  # path leading to training set\n",
    "        target_size=(64, 64), \n",
    "        batch_size=32, \n",
    "        class_mode='binary') # cat/dog--> binary outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Directory: Directory where the data is located. If labels is \"inferred\", it should contain subdirectories, each containing      images for a class.\n",
    "   main_directory/\n",
    "####   ...class_a/\n",
    "   ......a_image_1.jpg\n",
    "   ......a_image_2.jpg\n",
    "####   ...class_b/\n",
    "   ......b_image_1.jpg\n",
    "   ......b_image_2.jpg\n",
    "2. target_size: Tuple of integers (height, width), defaults to (256, 256). The dimensions to which all images found will be        resized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating images for the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten,Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\", input_shape=[64, 64, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a second convolutional layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Conv2D(filters=32, kernel_size=3, activation=\"relu\"))\n",
    "cnn.add(MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "=================================================================\n",
      "Total params: 10,144\n",
      "Trainable params: 10,144\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one dimension flattened layer will be the i/p to the fully connected Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=128,activation='relu'))  # units=Number of hidden neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(Dense(units=1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 250s 998ms/step - loss: 0.6784 - accuracy: 0.5723 - val_loss: 0.6845 - val_accuracy: 0.6650\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 91s 365ms/step - loss: 0.6250 - accuracy: 0.6456 - val_loss: 0.6400 - val_accuracy: 0.6850\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 90s 360ms/step - loss: 0.5732 - accuracy: 0.7009 - val_loss: 0.4923 - val_accuracy: 0.7325\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 93s 373ms/step - loss: 0.5454 - accuracy: 0.7214 - val_loss: 0.4272 - val_accuracy: 0.7340\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 89s 354ms/step - loss: 0.5109 - accuracy: 0.7431 - val_loss: 0.5626 - val_accuracy: 0.7045\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.4880 - accuracy: 0.7590 - val_loss: 0.5734 - val_accuracy: 0.7400\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 90s 358ms/step - loss: 0.4816 - accuracy: 0.7666 - val_loss: 0.4286 - val_accuracy: 0.7685\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 90s 361ms/step - loss: 0.4583 - accuracy: 0.7791 - val_loss: 0.3061 - val_accuracy: 0.7800\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 88s 354ms/step - loss: 0.4360 - accuracy: 0.7956 - val_loss: 0.3824 - val_accuracy: 0.7750\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.4308 - accuracy: 0.7950 - val_loss: 0.2869 - val_accuracy: 0.7890\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 90s 359ms/step - loss: 0.4166 - accuracy: 0.8098 - val_loss: 0.5686 - val_accuracy: 0.8060\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.3987 - accuracy: 0.8152 - val_loss: 0.5098 - val_accuracy: 0.7820\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 88s 351ms/step - loss: 0.3885 - accuracy: 0.8214 - val_loss: 0.4009 - val_accuracy: 0.7590\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.3649 - accuracy: 0.8342 - val_loss: 0.6438 - val_accuracy: 0.7915\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.3504 - accuracy: 0.8461 - val_loss: 0.3271 - val_accuracy: 0.8010\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 89s 356ms/step - loss: 0.3400 - accuracy: 0.8509 - val_loss: 0.3199 - val_accuracy: 0.7860\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 89s 356ms/step - loss: 0.3283 - accuracy: 0.8537 - val_loss: 0.3149 - val_accuracy: 0.7900\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.3080 - accuracy: 0.8655 - val_loss: 0.3559 - val_accuracy: 0.8100\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 87s 350ms/step - loss: 0.2910 - accuracy: 0.8734 - val_loss: 0.5020 - val_accuracy: 0.8060\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.2801 - accuracy: 0.8852 - val_loss: 0.2304 - val_accuracy: 0.8045\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 87s 350ms/step - loss: 0.2652 - accuracy: 0.8878 - val_loss: 0.2194 - val_accuracy: 0.7950\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 88s 353ms/step - loss: 0.2516 - accuracy: 0.8954 - val_loss: 0.4456 - val_accuracy: 0.7870\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.2386 - accuracy: 0.9044 - val_loss: 0.9460 - val_accuracy: 0.8020\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 92s 368ms/step - loss: 0.2242 - accuracy: 0.9055 - val_loss: 0.5697 - val_accuracy: 0.8080\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 88s 352ms/step - loss: 0.2268 - accuracy: 0.9106 - val_loss: 0.1719 - val_accuracy: 0.8050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2282cc73320>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Making Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image=image.load_img('dataset/single_prediction/download.jpg',target_size=(64,64))\n",
    "test_image=image.img_to_array(test_image)\n",
    "test_image=np.expand_dims(test_image, axis=0)\n",
    "result=cnn.predict(test_image)  \n",
    "if result[0][0]==1:\n",
    "    prediction='dog'\n",
    "else:\n",
    "    prediction='cat'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog\n"
     ]
    }
   ],
   "source": [
    "print(prediction) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. target_size is adjusted her as while bulding the cnn, we have made the target size as (64,64)\n",
    "2. The predict methods expects 2d array, hence you need to covert the image to 2D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you trained your model on mini-batches, your input is a tensor of shape [batch_size, image_width, image_height, number_of_channels].\n",
    "\n",
    "When predicting, you have to respect this shape even if you have only one image. Your input should be of shape: [1, image_width, image_height, number_of_channels].\n",
    "\n",
    "You can do this in numpy easily. Let's say you have a single 5x5x3 image:\n",
    "\n",
    "    >>> x = np.random.randint(0,10,(5,5,3))\n",
    "    >>> x.shape\n",
    "    >>> (5, 5, 3)\n",
    "    >>> x = np.expand_dims(x, axis=0)\n",
    "    >>> x.shape\n",
    "    >>> (1, 5, 5, 3)\n",
    "\n",
    "Adding batch as 1st dimension, hence axis=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(1, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "A=image.load_img('dataset/single_prediction/cat_or_dog_1.jpg',target_size=(64,64))\n",
    "A=image.img_to_array(A)\n",
    "print(A.shape)\n",
    "A=np.expand_dims(A, axis=0)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " result is also contained in a batch since, test_image is passed in a batch \n",
    " \n",
    " \n",
    " therfore we have to first get access to the batch(in this case there is only one batch): Index 0\n",
    " \n",
    " Inside the batch, we will get access to the first and only element in the batch which corresponds to the prediction\n",
    " and has Index 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
